<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2020-07-13T14:09:04-06:00</updated><id>/</id><title type="html">Massively</title><subtitle>Jekyll version of the Massively theme by HTML5UP</subtitle><entry><title type="html">Computer Vision Basics</title><link href="/blog/computer-vision-basics/" rel="alternate" type="text/html" title="Computer Vision Basics" /><published>2020-07-05T00:00:00-06:00</published><updated>2020-07-05T00:00:00-06:00</updated><id>/blog/computer-vision-basics</id><content type="html" xml:base="/blog/computer-vision-basics/">&lt;h1 id=&quot;the-basics-of-computer-vision&quot;&gt;The Basics of Computer Vision&lt;/h1&gt;
&lt;p&gt;Computer vision can be an intimidating field, as it is arguably difficult, and math intensive. It encompasses subfields such as geometry, linear algebra, and machine learning, among others and can take a long time to master. While there is no shortage of material on the internet telling you how to “build” a state of the art image classifier in 5 lines of code or less, these tutorials hide the details of how these systems work. While these tutorials are good for some audiences who just want a quick solution to their problems, they can often be detremental by depriving the users of insights into why things break. Without these insights, most people will just give up on a solution, when minor alterations to the underlying algorithm could solve many of their problems.&lt;/p&gt;

&lt;p&gt;This post is intended to briefly describe some important concepts to give new developers an idea about what tools more sophistocated algorithms are using behind the scenes. Many of these topics are active fields of research, and if there is enough interest, I can go over them in more detail at a later date.&lt;/p&gt;

&lt;h2 id=&quot;kernels&quot;&gt;Kernels&lt;/h2&gt;
&lt;p&gt;Gaussian Blur, Edge detectors, Corner Detectors, etc&lt;/p&gt;

&lt;h2 id=&quot;feature-detectors&quot;&gt;Feature Detectors&lt;/h2&gt;
&lt;p&gt;DOG, HOG, SIFT, SURF, ORB, etc&lt;/p&gt;

&lt;h2 id=&quot;camera-calibration&quot;&gt;Camera Calibration&lt;/h2&gt;
&lt;p&gt;Camera Matrix K, Distortion Coefficients, Extrinsics Calibration&lt;/p&gt;

&lt;h2 id=&quot;epipolar-geometry&quot;&gt;Epipolar Geometry&lt;/h2&gt;
&lt;p&gt;Stereo Rectification, Epipolar Lines, Triangulation/Project and Unproject operations.&lt;/p&gt;

&lt;h2 id=&quot;filtering&quot;&gt;Filtering&lt;/h2&gt;
&lt;p&gt;RANSAC, Outlier Rejection&lt;/p&gt;

&lt;h2 id=&quot;3d-reconstruction&quot;&gt;3D Reconstruction&lt;/h2&gt;
&lt;p&gt;ICP, Voxel Grids, Surfels&lt;/p&gt;

&lt;h2 id=&quot;pyramiding&quot;&gt;Pyramiding&lt;/h2&gt;
&lt;p&gt;Upscale and downscale, lead into application for optical flow&lt;/p&gt;

&lt;h2 id=&quot;optical-flow&quot;&gt;Optical Flow&lt;/h2&gt;
&lt;p&gt;Simple feature tracking over frames, as well as dense methods&lt;/p&gt;

&lt;h2 id=&quot;deep-neural-networks&quot;&gt;Deep Neural Networks&lt;/h2&gt;
&lt;p&gt;Address the obnoxious figures that all of the online tutorials put up without any explanation.&lt;/p&gt;</content><summary type="html">In this post, I go over some basic computer vision concepts to give developers an idea of what goes on behind the scenes of more sophistocated algorithms</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/pic02.jpg" /></entry><entry><title type="html">Wheelchair</title><link href="/blog/wheelchair/" rel="alternate" type="text/html" title="Wheelchair" /><published>2020-02-15T00:00:00-07:00</published><updated>2020-02-15T00:00:00-07:00</updated><id>/blog/wheelchair</id><content type="html" xml:base="/blog/wheelchair/">&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/T1uub8PVldE&quot; width=&quot;700&quot; height=&quot;480&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Over the past few months, I have built a proof of concept semi autonomous wheelchair from the ground up. By mounting a depth camera to my wheelchair, I can construct 3D maps of my surroundings which can be used for more sophistocated obstacle detection and navigation tasks. I created a custom Gazetracking solution which operates using a simple web camera and allows you to control your computer cursor with just your eyes. A user can drive my wheelchair by either using a virtual joystick, or by selecting waypoints on the generated map, and letting the wheelchair autonomously take you to your destination.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Progress in the field of robotics is impressive. We are already seeing its impacts in warehouses, factories, and some individuals promise us that fully self driving cars are right around the corner. Unfortunately, the majority of the commerical efforts in this field are relatively separated from public view. One demograhic which could especially benefit from the robotics revolution is the group of people suffering from serious physical disabilities. According to the Reeve Foundation, nearly 1 in 50 people are living with some form of paralysis. My hope is that by developing new assistive robotic technologies, we can have a meaningful impact in the lives of millions of people around the world.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;
&lt;p&gt;The first version of my wheelchair consists of a 3D printed control mount and RGB-D camera which can be easily attached to the joystick controller of most power wheelchairs. This system gives the user the ability to perform both manual and semi-autonomous navigation. Through the use of a graphicalThe users of this system currently control all navigation aspects through a graphical user interface on a computer screen.&lt;/p&gt;

&lt;p&gt;This graphical interface has three main components. The first component is a virtual joystick with buttons for directly telling the wheelchair to move forward, backward, left, or right. The second component provides the user with a display of what the camera sensor can see. This component allows the user to simply click on any point in view, and the wheelchair will navigate there autonomously. The final component displays the map generated from the computer vision system. The user can simply click on any part of the map, and the chair will attempt to autonomously navigate to the desired location.&lt;/p&gt;

&lt;p&gt;The graphical interface I am using for my wheelchair provides a method of control, but it is still impossible for individuals with significant paralysis, such as Stephen Hawking, to use. To get around this problem, I built a gaze tracking tool which can accurately predict to within around half an inch (on a 15 inch computer monitor) which pixel a user is looking at. If this level is not sufficient for your task, it also has a mode for fine grained control to more accurately select smaller targets. This tool is configured to directly control your mouse using a simple web camera. For my specific application, I decided to use an infrared camera so that the user can still operate in the dark.&lt;/p&gt;

&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/z8XSkEGeyec&quot; width=&quot;700&quot; height=&quot;480&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;

&lt;h2 id=&quot;future-plans&quot;&gt;Future Plans&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Custom Hardware:&lt;/strong&gt;  As I mentioned above, this wheelchair is currently just a proof of concept; it can demonstrate specific key features, but is not yet reliable enough to be a commercial product. I currently believe that the best way forward for this project is to build a wheelchair from the ground up. This will allow me to increase the amount of power available to the system, include accurate motors with wheel encoders for more reliable position control, and a tightly integrated sensor array that can accurately perceive the 360 degree environment around it. Moreover, it will also allow me to create a more aesthetically pleasing product.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improved Autonomy:&lt;/strong&gt; My wheelchair is currently semi autonomous, meaning that it can attempt to navigate a path between two points, but will still require user intervention to prevent collisions. With a more advanced sensor array, I will be able to work on implementing more advanced autonomous features such as navigating around people in a moving airport, following a friend, or moving up a ramp and into a car. This behavior will make it easier and safer for users of my wheelchair to navigate and multitask. Moreover, a fully autonomous chair might have commercial applications in realms such as short distance ride sharing, and the transportation of patients around hospitals.&lt;/li&gt;
&lt;/ul&gt;</content><summary type="html">A Semi-Autonomous Wheelchair that you can control with just your eyes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/wheelchair_cover_new.png" /></entry><entry><title type="html">Gazetracker</title><link href="/blog/gazetracker/" rel="alternate" type="text/html" title="Gazetracker" /><published>2020-02-15T00:00:00-07:00</published><updated>2020-02-15T00:00:00-07:00</updated><id>/blog/gazetracker</id><content type="html" xml:base="/blog/gazetracker/">&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/JKsqNEyIFl4&quot; width=&quot;700&quot; height=&quot;480&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;I built a tool that allows you to control your computor cursor with just your eyes. You just look at the pixel you want to move to, and wink to click. This tool works with simple webcameras rather than expensive commercial hardware.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In extreme cases of paralysis, people can completely lose all motor functions, as well as their ability to speak. To the best of my knowledge, there are very few solutions which would allow these people to interact with technology, let alone anything else in the world around them. As I worked on building out my self driving wheelchair project, I discovered that even if I purchased a commercial eye tracker, it restricted development without a license. In order to obtain a robust solution without legal limitations, I decided to build my own gazetracker from the ground up.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;
&lt;p&gt;My gazetracking solution is currently capable of predicting which pixel you are looking at to within around 1/2 an inch on a 15 inch screen. For when that level of precision is not enough, I created a high precision mode. Here, the cursor will remain stationary when you are looking within a user defined radius. When you look outside of the radius, the cursor will move towards your new target, slowly for something close, and more quickly for something farther away. Left and right click operations are mapped to winking with your left and right eye, respectively, and toggling in and out of high precision mode is mapped to closing both of your eyes for a few seconds.&lt;/p&gt;

&lt;p&gt;Unlike commercial solutions, my gazetracker uses a simple webcam for all operations. It is able to function whether the user is wearing glasses or not, and it is also capable of consistently predicting the same target, even as the user moves their head.&lt;/p&gt;

&lt;h2 id=&quot;future-plans&quot;&gt;Future Plans&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Develop Custom Interfaces:&lt;/strong&gt; While my solution allows users to interact with a computer, the level of precision I provide can still make it a somewhat difficult and time consuming process for certain tasks. I would like to explore creating a more intelligent cursor which displays a circular uncertainty radius. My proposed system would then intelligently select a target from within that radius, thus reducing the level of precision required by the user.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Calibration Methods:&lt;/strong&gt; My gazetracker currently works very well for my system configuration, however, most webcameras have different focal lengths, distortion coefficients, and can be placed in various positions around monitors which can also come in varous shapes and sizes. I need to come up with a solution which will allow my gazetracking software to work reliably with other machines.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Verification on Other Users:&lt;/strong&gt; I developed my gazetracking solution in isolation as I was socially distancing myself for the COVID-19 pandemic. I still need to verify that my solution will work well for a diverse group of individuals.&lt;/li&gt;
&lt;/ul&gt;</content><summary type="html">A new solution for fast and accurate control of your cursor with just your eyes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/gazetracker_header.png" /></entry><entry><title type="html">Konstruct</title><link href="/blog/konstruct/" rel="alternate" type="text/html" title="Konstruct" /><published>2020-02-15T00:00:00-07:00</published><updated>2020-02-15T00:00:00-07:00</updated><id>/blog/konstruct</id><content type="html" xml:base="/blog/konstruct/">&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/FHx5oGUPqlc&quot; width=&quot;700&quot; height=&quot;480&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Using Unity 3D, I created a tool for the large scale generation of training data for computer vision tasks. With the use of some outside libraries such as ROS-Sharp, I found that this tool could be very helpful as a simulation environment for robotics.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;As I taught myself the principles of deep learning, I found immensely frustrating that I was constrained to working with a collection pre-existing datasets. As I was training my image classifiers, I knew that I was already trying to tackle a solved problem, and I wanted to see if I could get my models to work on something new. Unfortunately, it was very difficult and impreactical to go off and build new datasets with traditional approaches. For tasks such as image segmentation, one could spend days or weeks manually annotating images, and still not have enough data to train anything useful. Moreover, annotating data by hand is incredibly error prone which can result in poor performance in your models later on. In an effort to get around this problem, I decided to use my knowledge of computer graphics to generate large scale datasets with perfect annotations.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;
&lt;p&gt;Konstruct is a simulation environment based on Unity3D which can be used to generate large, perfectly annotated pose and image datasets. In order to run, the uesr must provide 3D models of their objects of interest. Konstruct will then endlessly generate randomized scenes of these objects and capture images from various angles. Using a set of shaders, I am able to produce a depth map, as well as a perfect segmentation mask of the various objects in each scene. In addition to depth and sementation data, the ground truth 6 degree of freedom pose of each object in the scene, including the camera, is recorded to a JSON file.&lt;/p&gt;

&lt;p&gt;More recently, I have also started using the ROS-Sharp plugin for Unity. This plugin allows you to run robotics code directly in simulation. This feature is incredibly useful because it allows you to evaluate the performance and shortcomings of your system in a world with perfect information.&lt;/p&gt;

&lt;h2 id=&quot;future-plans&quot;&gt;Future Plans:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Domain Randomization:&lt;/strong&gt; While Konstruct can currenlty provide information about objects placed randomly throughout a scene, there are several tricks which can be used to generate far more diverse datasets. For example, by varying lighting conditions, adding stickers to simulate stains or scuffs, and changing colors and texture properties of the objects, it should be possible to generate a far more diverse datset with little effort. This will in turn lead to the development of models which generalize better to the real world.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3D Reconstruction:&lt;/strong&gt; I would like to eventually include a variant of Kinect Fusion in this library to make it easier new users to reconstruct models to load into the simulation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;current-uses&quot;&gt;Current Uses&lt;/h2&gt;
&lt;p&gt;Right now, I am actively using the Konstruct in the development of my semi autonomous wheelchair. This tool was incredibly helpful when I was developing a proof of concept solution without any physical hardware. Now that I have built the first version of my wheelchair, I am working to train new deep learning models on segmentation data to augment my navigation costmap.&lt;/p&gt;
&lt;div class=&quot;embed-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/vDqv2U0Zu9E&quot; width=&quot;700&quot; height=&quot;480&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;
    &lt;/iframe&gt;
  &lt;/div&gt;</content><summary type="html">A set of tools for generating simulation data using Unity 3D</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/konstruct_cover.png" /></entry><entry><title type="html">Ultimate Gridworld</title><link href="/blog/gridworld/" rel="alternate" type="text/html" title="Ultimate Gridworld" /><published>2019-05-20T00:00:00-06:00</published><updated>2019-05-20T00:00:00-06:00</updated><id>/blog/gridworld</id><content type="html" xml:base="/blog/gridworld/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;I built a new, easily extensible, gridworld simulation environment from scratch targeted for large scale research in the field of human robot collaboration.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;I decided to start this project when I saw my friends and new PhD students in the CAIRO Lab at CU Boulder attempting to conduct experiments by repurposing various open source projects. I decided to start work, from the ground up, on a simulation environment that could be easily extended to suit their needs, and also be scaled substantially to generate convincing datasets which would verify their ideas.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;
&lt;p&gt;The target audience for ultimate gridworld requested that they could create a simple floorplan which to represent a building. This floorplan needed to have features such as exits to represent the goals of a character, or agent, as well as fires to represent obstacles.&lt;/p&gt;

&lt;p&gt;As there was an active interest in the use of this tool, I decided to use Object Oriented design principles to meet their needs, as well as to make this environment easily extensible for future projects. I created features such as abstract obstacle classes which would allow the users to easily add obstacles or rewards with various characteristics. I also created abstract classes and methods which would allow for the creation of many unique environments with characteristics such as long narrow hallways, or Large open spaces, and everything in between.&lt;/p&gt;

&lt;p&gt;One of the more unique, targeted features of this environment was the automated generation of planning predicates (TODO: CITATION NEEDED), which could be used to describe the different regions of the gridworld with varying levels of granularity. These planning predicates were again created using abstraction layers which will allow future users to efficiently taylor the characteristics of their environments to there specific needs.&lt;/p&gt;

&lt;p&gt;The initial intended use of these simulation environments is to simulate emergency evacuation scenarios. For two proposed experiments, this meant that multiple representations of the same environment needed to be maintained. For example, one agent may believe that gridcell A5 is on fire, and another may believe that gridcell C6 is on fire, when in fact only gridcell B5 is on fire. These different representations can have significant impacts on the ways different agents behave, and thanks to a layered implementation, it is a simple matter for new users to set up such scenarios.&lt;/p&gt;

&lt;p&gt;When exploring the behaviors of autonomous agents, there needs to be a protocol for these agents to navigate. My base implementation uses A* for these agents to naviate to the various exits, but my implementation allows the a user to simply define a new algorithm and pass it as an argument to the agent.&lt;/p&gt;

&lt;p&gt;It is important to be able to visualize how an environment changes at each timestep. For example, you may want to see how a fire spreads through the maze, or the steps that an agent takes. I currently provide this visualization by printing each gridworld as colored text to the terminal at each timestep. For example, the walls could be represented as a Green W, fires represented as a Red F, the path the agent took (from start to finish) have a yellow background. This visualization method maintains a lightweight tool with limited outside dependencies, although a better graphical interface, perhaps using Qt should be explored in the future.&lt;/p&gt;

&lt;h2 id=&quot;ultimate-gridworld-in-use&quot;&gt;Ultimate Gridworld in Use&lt;/h2&gt;
&lt;p&gt;Ultimate gridworld has already been used in the development and analysis of the SPEAR algorithm (TODO: ADD LINK TO PAPER IN ARXIV) This algorithm, was profiled and shown to achieve an order of magnitude performance boost over the state of the art performance over the course of thousands of unique worlds thanks to my efforts with this repository.&lt;/p&gt;</content><summary type="html">An Easily extensible grid based maze generator with obstacles</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/images/gridworld_cover.png" /></entry></feed>
